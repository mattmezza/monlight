# Progress & Learnings

## Session 1: Repository Setup

- The monorepo has 5 top-level service directories: `error-tracker/`, `log-viewer/`, `metrics-collector/`, `clients/python/`, `shared/`
- Deployment config lives in `deploy/` with `docker-compose.monitoring.yml` and `secrets.env.example`
- `deploy/data/{errors,logs,metrics}/` directories hold SQLite persistence volumes
- The docker-compose file uses relative build paths (`../error-tracker`, etc.) since it lives in `deploy/`
- The `.gitignore` must cover: Zig (`zig-out/`, `zig-cache/`, `.zig-cache/`), SQLite (`*.db`, `*.db-wal`, `*.db-shm`), Python (`__pycache__/`, `*.egg-info/`, etc.), env files (`secrets.env`, `.env`), and Node (`node_modules/`)
- All three services map internal port 8000 to external ports 5010, 5011, 5012 respectively
- Services connect via the `flowrent_network` external Docker network
- Each service binary supports a `--healthcheck` CLI flag for Docker health checks
- The `env_file` directive in docker-compose points to `secrets.env` (not committed; `secrets.env.example` serves as template)
- Environment variable naming convention: service-specific keys use service name prefix (e.g., `ERROR_TRACKER_API_KEY`), but each service reads it as just `API_KEY` from its own config module

## Session 2: Error Tracker Bootstrap & SQLite Layer

- Zig 0.13 is installed at `/tmp/zig-linux-x86_64-0.13.0/zig` (not in PATH by default)
- In Zig 0.13, `std.Build.Module` does NOT have a `link_library` field. C library linking must be done on the artifact (executable/test) only via `linkSystemLibrary` and `linkLibC`.
- In Zig 0.13, `std.http.Server.Request.respond()` signature is `respond(content, options)` — content ([]const u8) comes FIRST, options struct second. This differs from some later versions.
- The `@cImport` of `sqlite3.h` works with system-installed `sqlite-dev` package on Alpine and `libsqlite3-dev` on Debian/Ubuntu.
- Shared modules (e.g., `shared/sqlite.zig`) are imported into services via `b.addModule("sqlite", .{ .root_source_file = b.path("../shared/sqlite.zig") })` in `build.zig`, then `exe.root_module.addImport("sqlite", sqlite_mod)`.
- Since `build.zig` references `../shared/`, the Dockerfile must use the repo root as Docker build context, not the service directory. Updated docker-compose to use `context: ..` with `dockerfile: error-tracker/Dockerfile`.
- SQLite `execMulti` uses an internal 8KB buffer for null-terminating SQL strings. Migration SQL must fit within this limit.
- In-memory SQLite databases (`:memory:`) use `memory` journal mode, not WAL. Tests that check WAL pragma should account for this.
- SQLite `EXPLAIN QUERY PLAN` returns the detail string in column index 3 — useful for verifying index usage in tests.
- The `_meta` table pattern for schema versioning: `(key TEXT PRIMARY KEY, value TEXT)` with `schema_version` key stores the version as a text string. Migration runner compares version as `usize` integer.

## Session 3: Error Tracker Configuration Module

- Shared config module (`shared/config.zig`) provides generic env var parsing helpers: `getRequired`, `getString`, `getInt`, `getBool`, `getOptional`, `initLogLevel`. Each service's config module uses these to define its own specific config struct.
- Adding a new shared module requires updates in `build.zig`: (1) `b.addModule(...)` to create the module, (2) `exe.root_module.addImport(...)` on the executable, and (3) the same addImport on EVERY test artifact that transitively imports it.
- The shared config module does NOT need `linkSystemLibrary("sqlite3")` or `linkLibC()` since it has no C dependencies — only test artifacts with C deps need those.
- Environment variables in Zig tests cannot be easily set/unset, so config tests rely on verifying behavior with env vars known to NOT exist (e.g., `MONLIGHT_TEST_NONEXISTENT_VAR_12345`) rather than setting them.
- `std.debug.print` is used for fatal startup messages (instead of `std.log`) because log level filtering might suppress the message. Fatal config errors must always be visible.
- Config struct stores a `db_path_z: [*:0]const u8` (null-terminated) alongside the regular `database_path: []const u8` slice, because SQLite's C API requires null-terminated strings. The null-terminated version is backed by a `_db_path_buf: [512]u8` field within the struct.
- IMPORTANT: Previous feature branches are not auto-merged into main. When branching for a new task, branch off the PREVIOUS feature branch (not main) if that branch contains required code (e.g., `shared/sqlite.zig`).
- `std.posix.getenv()` returns `?[]const u8` — an optional slice. It returns null when the env var is not set.

## Session 4: Error Tracker Auth Integration Tests

- Integration tests for HTTP behavior (auth, routing) require spinning up a real TCP server because `std.http.Server.Request` cannot be mocked — it requires a real connection.
- Pattern for HTTP integration tests: create a `TestServer` struct that owns the `net.Server`, start it with a thread that accepts N connections, send raw HTTP requests, parse raw responses. Use port 0 to let the OS assign a free port (avoids conflicts in parallel test runs).
- `handleConnection` must be `pub` in main.zig so that test files (`auth_test.zig`) can import and call it.
- Test files that import `main.zig` (via `@import("main.zig")`) need all the same shared module imports and C library linking as `main.zig` itself in `build.zig`.
- Zig compiler rejects `_ = self;` if `self` was already used earlier in the function — the discard is "pointless" and causes a compile error.
- Deleting `.zig-cache/o/` partially can corrupt the build cache. If the build produces `FileNotFound`, delete the entire `.zig-cache/` directory and rebuild.

## Session 5: Error Tracker Request Limits (Rate Limiting + Body Size)

- In Zig 0.13, the HTTP 413 status code enum is `.payload_too_large`, NOT `.request_entity_too_large`. Always check `/tmp/zig-linux-x86_64-0.13.0/lib/std/http.zig` for the correct enum names.
- The middleware pattern used across the codebase is "call-and-check": each middleware function takes `*std.http.Server.Request` plus its config, sends the error response itself if rejected, and returns an enum the caller checks. This avoids function chaining complexity.
- When adding a new shared module (e.g., `rate_limit`), you must add `addImport` to: (1) the main executable, (2) ALL test artifacts that transitively import it (including test files that `@import("main.zig")` since main.zig now imports the new module).
- Rate limiter uses a ring buffer of timestamps for sliding window tracking. The `max_requests` parameter doubles as the ring buffer size, keeping memory usage proportional to the limit.
- `handleConnection` signature changes (adding parameters) require updating ALL test files that call it. Currently `auth_test.zig` and `rate_limit_test.zig` both call `main.handleConnection(...)`.
- For integration tests of rate limiting, use a `TestServer` that owns the `RateLimiter` with a small limit (e.g., 2-3 requests) to make tests fast and deterministic. Auth tests should use a generous limit (e.g., 1000) so rate limiting doesn't interfere.
- Body size enforcement checks the `Content-Length` header before reading the body, avoiding memory allocation for oversized requests. Requests without `Content-Length` are allowed through (they may have no body or use chunked encoding).

## Session 6: Error Tracker Fingerprinting Algorithm

- The fingerprint module (`fingerprint.zig`) is pure Zig — no C deps (uses `std.crypto.hash.Md5`), no shared module deps. Test artifacts for it don't need `linkSystemLibrary` or `linkLibC`.
- Zig 0.13's `std.crypto.hash.Md5` API: `Md5.init(.{})` → `.update(data)` → `.final(&digest)`. Digest is `[16]u8`. Also has a one-shot `Md5.hash(input, &out, .{})`.
- Python traceback parsing: the `File "...", line N` format always has `", line "` (with the comma+space) immediately after the closing quote of the file path. The optional `, in function_name` part comes after the line number.
- The fingerprint key format is `{project}:{exception_type}:{file}:{line}` — the MD5 hasher can be fed these parts incrementally with `.update()` calls (including the `:` separators) rather than needing to allocate a concatenated string.
- When no traceback location can be parsed (non-Python tracebacks or plain text), the fallback uses the entire traceback string as the location component, still producing a deterministic 32-char hex fingerprint.

## Session 7: Error Tracker Error Ingestion Endpoint

- The `error_ingestion.zig` module handles: JSON parsing/validation, fingerprint-based deduplication (create/increment/reopen), occurrence record creation, and occurrence trimming (max 5 per error).
- `std.json.parseFromSliceLeaky` is preferred over `parseFromSlice` when using an arena allocator — it skips tracking individual allocations since the arena will free them all at once.
- The ingestion flow: parse JSON → validate required fields + length limits → compute fingerprint → find existing error by fingerprint → create/increment/reopen → create occurrence → trim occurrences. All DB operations are sequential (no transactions needed for single-row operations).
- `handleConnection` signature grew to include `db: *sqlite.Database` and `cfg: *const app_config.Config` so the error ingestion handler can access the database and config (e.g., Postmark settings for email alerts).
- When `handleConnection` signature changes, ALL test files that call it must be updated. Test files need `makeTestConfig()` helper functions that create dummy Config structs with valid field values.
- The `Config` struct has a `_db_path_buf: [512]u8` field that backs the null-terminated `db_path_z` pointer. When creating test configs, this buffer must be properly initialized with `@memcpy` and null terminator.
- Body size enforcement tests that previously used `POST /api/errors` had to be changed to `POST /api/test` (which returns 404) because the error ingestion handler now tries to read the request body. If `Content-Length` claims N bytes but none are sent, the server hangs waiting for data.
- The Postmark email integration builds JSON payloads manually using `std.ArrayList(u8)` writer with a `writeJsonEscaped` helper that escapes `"`, `\\`, `\n`, `\r`, `\t`, and control chars below 0x20.
- `std.http.Client` in Zig 0.13: create with `{ .allocator = allocator }`, `open(.POST, uri, .{ .server_header_buffer, .extra_headers })`, set `.transfer_encoding = .{ .content_length = len }`, then `send()` → `writeAll(payload)` → `finish()` → `wait()`.
- Email alerting is fire-and-forget: failures are logged with `log.warn` but never propagate errors back to the HTTP response. If Postmark credentials are not configured (`null`), the alert is skipped silently with an early return.

## Session 8: Error Listing Endpoint

- The `error_listing.zig` module handles GET /api/errors with query parameter parsing, dynamic SQL construction, and JSON response formatting.
- Since Zig doesn't support runtime string concatenation for SQL queries easily, the approach uses 4 separate prepared statement variants (no filters, project only, environment only, both) rather than dynamically building a single SQL string. This is verbose but safe and avoids SQL injection.
- Query parameter parsing uses `std.mem.splitScalar` to split on `&`, then `std.mem.indexOf` to find `=` separators. No URL decoding is implemented yet (not needed for the current use case).
- `std.fmt.parseInt(u32, value, 10)` is used for parsing numeric query params like `limit` and `offset`, with `catch continue` to skip invalid values gracefully.
- The listing endpoint returns a two-query pattern: first a `SELECT COUNT(*)` for the total, then a `SELECT ... LIMIT ? OFFSET ?` for the paginated data. Both queries use the same filter conditions.
- Routing in `main.zig` distinguishes GET vs POST on `/api/errors` by checking the HTTP method after matching the path prefix. The `isApiErrorsPath()` helper matches both `/api/errors` (exact) and `/api/errors?...` (with query string).
- JSON response is built manually with `std.ArrayList(u8)` writer rather than using `std.json` serialization, to have full control over the output format and avoid needing to define Zig structs that match the exact JSON shape.

## Session 9: Error Detail Endpoint

- The `error_detail.zig` module handles GET /api/errors/{id} — returns full error group data plus the last 5 occurrences with request context.
- Routing in `main.zig`: within the GET `/api/errors` branch, `error_detail.extractId(target)` is checked first. If it returns an ID, the request is routed to `handleErrorDetail`; otherwise it falls through to `handleErrorListing`.
- `isApiErrorsPath()` was updated to also match `/api/errors/` sub-paths by adding a `rest[0] == '/'` check.
- Zig 0.13 type coercion quirk: `ArrayList(u8).toOwnedSlice()` returns `![]u8`, which does NOT implicitly coerce to `!?[]const u8` (optional + const). You must assign to an intermediate `const result: []const u8 = try buf.toOwnedSlice();` and then `return result;` — the `[]const u8` → `?[]const u8` coercion works, but `[]u8` → `?[]const u8` does not coerce through error union payloads directly.

## Session 10: Error Tracker Resolve Endpoint

- The `error_resolve.zig` module handles POST /api/errors/{id}/resolve — marks an error as resolved with `resolved=1` and `resolved_at` timestamp.
- Routing for POST resolve must be checked BEFORE the GET `/api/errors/{id}` branch in `main.zig`, since `isApiErrorsPath()` would also match `/api/errors/42/resolve`. The resolve check uses `error_resolve.extractResolveId()` which specifically matches the `/api/errors/{id}/resolve` pattern.
- The `extractResolveId` function is separate from `extractId` in `error_detail.zig` — it specifically looks for the `/resolve` suffix after the ID segment. This is cleaner than trying to make `extractId` handle both cases.
- Idempotent resolve: if the error is already resolved, the endpoint returns 200 without modifying `resolved_at`. This avoids updating the timestamp on every call and preserves the original resolution time.
- The resolve endpoint uses a tagged union `ResolveResult` (`.resolved` or `.not_found`) rather than an optional, making the handler code in `main.zig` clearer about which HTTP status code to return.
- Tests for `API_KEY`-dependent modules (main.zig, auth_test.zig, rate_limit_test.zig) require setting `API_KEY=test_key_123` in the environment when running `zig build test`.

## Session 11: Projects Listing Endpoint

- The `projects_listing.zig` module handles GET /api/projects — returns distinct project names from the errors table as `{"projects": ["flowrent", ...]}`.
- `Statement.deinit()` and `RowIterator.deinit()` both call `sqlite3_finalize` on the same underlying statement pointer. Calling both causes a double-free segfault. Pattern: only call `stmt.deinit()`, never `iter.deinit()` when you own the statement. This is the correct ownership pattern throughout the codebase.
- Routing for `/api/projects` must be placed BEFORE the `/api/errors` catch-all in `main.zig`, since `isApiErrorsPath()` would not match it, but ordering still matters for clarity and to avoid future conflicts with broader path matchers.
- The `isApiProjectsPath()` helper follows the same pattern as `isApiErrorsPath()`: matches `/api/projects` exactly or `/api/projects?...` with query string.
- The JSON response is built manually with `ArrayList(u8)` writer, consistent with the pattern established in `error_listing.zig` and `error_detail.zig`.

## Session 12: Data Retention Cleanup

- Retention cleanup uses a separate SQLite connection on a background thread. WAL mode supports concurrent read/write access from multiple connections, so this is safe alongside the main HTTP server thread.
- `std.atomic.Value(bool)` is used for thread stop signaling with `.load(.acquire)` / `.store(true, .release)` ordering. This ensures the background thread sees the stop signal promptly.
- SQLite's `strftime` with string concatenation `'-' || ? || ' days'` allows parameterized date arithmetic in DELETE queries (e.g., `datetime('now', '-' || ? || ' days')`).
- The `std.Thread.spawn` API in Zig 0.13 takes `.{}` (spawn config options), a function pointer, and an args tuple. The function pointer must match the args tuple types exactly.
- The retention thread sleeps in 1-second chunks rather than one long sleep, so it can check the stop flag frequently and shut down promptly.
- Associated `error_occurrences` are cascade-deleted via the existing `FOREIGN KEY (error_id) REFERENCES errors(id) ON DELETE CASCADE` constraint — no need to manually delete them.
- The retention module is tested with an in-memory SQLite database (`:memory:`), using the same schema setup pattern as other test modules. Tests insert errors with manually set timestamps to simulate aging.

## Session 13: Web UI for Error Tracker

- The Web UI is implemented as two HTML pages (`index.html` for listing, `error_detail.html` for detail view) embedded into the binary at compile time via `@embedFile("static/index.html")`.
- HTML files live in `error-tracker/src/static/` — Zig's `@embedFile` resolves paths relative to the source file that contains it, so `web_ui.zig` uses `@embedFile("static/index.html")`.
- Web UI routes (`GET /` and `GET /errors/{id}`) are handled **before** the auth middleware in `handleConnection`, so the HTML pages are served without requiring an API key. The JavaScript in the pages reads the API key from `localStorage` and passes it as `X-API-Key` header on fetch calls to the API endpoints.
- The `web_ui.zig` module contains `isErrorDetailPath()` which matches `/errors/{numeric_id}` — distinct from `/api/errors/{id}` which returns JSON.
- Tailwind CSS is loaded via CDN (`<script src="https://cdn.tailwindcss.com">`). The HTML is embedded in the binary but CSS is fetched at runtime. This avoids needing a Tailwind build pipeline in the Zig build system.
- The pages use a dark theme (gray-900 background) with color-coded elements: red for errors/production, yellow for staging/warning counts, green for resolved/development, blue for project badges, purple for user IDs.
- The listing page supports filtering by project (dropdown populated from `/api/projects`), environment, and resolved status. Pagination with prev/next buttons and offset tracking.
- The detail page shows traceback in a monospace `<pre>` block, occurrences in expandable `<details>` sections for headers and extra context, and a "Mark as Resolved" button that calls `POST /api/errors/{id}/resolve`.
- No additional build.zig changes are needed for `@embedFile` — Zig handles it automatically. The only build.zig change was adding the `web_ui_tests` target (no dependencies needed since web_ui.zig only uses std).

## Session 14: Metrics Collector (Full Implementation)

- **SQLite `datetime()` vs `strftime()` format mismatch**: SQLite's `datetime()` function outputs space-separated format (`2025-01-20 10:01:00`) while our timestamps use ISO 8601 with `T` separator (`2025-01-20T10:01:00Z`). This causes lexicographic comparison failures in `WHERE timestamp >= datetime(?, '+1 minute')`. Fix: always use `strftime('%Y-%m-%dT%H:%M:%SZ', ?, '+1 minute')` which outputs `T`-separated format matching our data.
- **Aggregation thread design**: The background aggregation thread combines three responsibilities: (1) minute-level aggregation every `aggregation_interval` seconds, (2) hour-level aggregation every 60 intervals (~hourly), and (3) retention cleanup also every 60 intervals. Each runs sequentially in the same thread with its own SQLite connection.
- **Percentile calculation**: Uses nearest-rank method — sort all values, compute index as `ceil(percentile/100 * count) - 1`, clamp to valid range. For hour-level aggregation, percentiles are approximated by averaging minute-level percentiles (exact would require storing all raw values).
- **metrics_query SQL building**: Dynamic SQL uses embedded offset strings from a fixed set of constants (`1 hour`, `24 hours`, `7 days`, `30 days`) — these come from our own `periodToOffset()` function, not from user input, so they're safe against injection without parameterization.
- **Web UI with uPlot charts**: The metrics dashboard uses uPlot (loaded via CDN) for two interactive charts: (1) Metric Explorer showing avg value + count over time for a selected metric, (2) Latency Percentiles showing p50/p95/p99 lines for histogram-type metrics. uPlot was chosen for its small size and fast canvas rendering.
- **Dashboard endpoint**: GET /api/dashboard returns summary stats (total_datapoints, distinct_metrics) and top 10 metrics by count. This is a simpler implementation than the plan specified (which called for error_rate, avg_latency_ms, etc.) — those specific fields assume particular metric naming conventions that may not exist yet.
- **Test counts**: metrics-collector has 8 test targets (main, database, config, ingestion, aggregation, retention, metrics_query, web_ui). All pass except the known config test (`API_KEY is missing` fails because API_KEY is set in the test environment). This same issue exists in all three services.
- **Total across all services**: error-tracker ~40 tests, log-viewer ~82 tests, metrics-collector ~60 tests. All passing with the single known config test exception per service.

## Codebase Patterns
- All three Zig services read `API_KEY` (not prefixed). In docker-compose, map from `secrets.env` prefixed vars: `API_KEY=${ERROR_TRACKER_API_KEY}`
- `setup_monlight()` is the recommended single-call entry point for wiring up both error tracking and metrics in FastAPI apps
- When testing with `httpx_mock` and `MetricsClient` shutdown flush, add `@pytest.mark.httpx_mock(can_send_already_matched_responses=True)` and mock the metrics endpoint too
- Python client package lives at `clients/python/` with `monlight/` as the importable package
- Use `pip install -e ".[dev]"` in a venv to install with all dev dependencies (pytest, fastapi, etc.)
- The Python client uses `httpx` for both async and sync HTTP calls
- `pyproject.toml` uses `setuptools.build_meta` backend (not the newer `setuptools.backends._legacy`)
- Use `pytest-httpx` for mocking HTTP calls: `httpx_mock.add_response()` / `httpx_mock.add_exception()` / `httpx_mock.get_requests()`
- Tests live in `clients/python/tests/` and are configured via `[tool.pytest.ini_options]` in pyproject.toml
- The `MonlightExceptionHandler` expects `ErrorClient` to be stored at `request.app.state.monlight_error_client` — use `setup_monlight()` to wire it up
- Shared infrastructure (http.zig, json.zig, html.zig, shutdown.zig, log.zig) was implemented inline in each service's main.zig rather than as separate shared modules — the functionality exists but not as separate reusable files
- Config tests for "API_KEY is missing" are environment-aware: they check if API_KEY is set, and verify the appropriate behavior (success or error). In CI, set `API_KEY=test_key_ci_12345` and `CONTAINERS=test_container` for all Zig tests.
- Docker builds for all services require repo root as build context (not the service directory) because Dockerfiles copy from `shared/`. Use `context: ..` + `dockerfile: <service>/Dockerfile` in docker-compose.
- Use `pip install -e ".[dev]"` in a venv to install with all dev dependencies (pytest, fastapi, etc.)
- The Python client uses `httpx` for both async and sync HTTP calls
- `pyproject.toml` uses `setuptools.build_meta` backend (not the newer `setuptools.backends._legacy`)
- Use `pytest-httpx` for mocking HTTP calls: `httpx_mock.add_response()` / `httpx_mock.add_exception()` / `httpx_mock.get_requests()`
- Tests live in `clients/python/tests/` and are configured via `[tool.pytest.ini_options]` in pyproject.toml
- The `MonlightExceptionHandler` expects `ErrorClient` to be stored at `request.app.state.monlight_error_client` — use `setup_monlight()` to wire it up
- Shared infrastructure (http.zig, json.zig, html.zig, shutdown.zig, log.zig) was implemented inline in each service's main.zig rather than as separate shared modules — the functionality exists but not as separate reusable files
- E2e smoke tests live in `deploy/`: run `docker compose -f docker-compose.test.yml up --build -d` then `./smoke-test.sh`. Uses ports 15010-15012 to avoid production conflicts.
- JS SDK lives at `clients/js/`, buildable with `npm run build`. Uses esbuild for bundling, vitest + jsdom for testing, TypeScript for type safety. Entry point: `src/core.ts`, public API: `init(config)` → `MonlightClient`.

- Created `clients/python/pyproject.toml` with setuptools build backend, httpx dependency, and optional fastapi/dev dependency groups
- Created package directory structure: `monlight/__init__.py`, `error_client.py`, `metrics_client.py`, `integrations/__init__.py`, `integrations/fastapi.py`
- Implemented full `ErrorClient` class with async `report_error()` and sync `report_error_sync()` methods, PII header filtering, fire-and-forget behavior
- Implemented full `MetricsClient` class with `counter()`, `histogram()`, `gauge()` methods, in-memory buffer, periodic flush via threading.Timer, and `shutdown()` method
- Implemented `MonlightMiddleware` (ASGI middleware for request metrics), `MonlightExceptionHandler` (global exception handler), and `setup_monlight()` convenience function
- Created `tests/test_scaffolding.py` with 7 tests verifying imports, constructor behavior, and URL normalization
- Files changed: `clients/python/pyproject.toml`, `clients/python/monlight/__init__.py`, `clients/python/monlight/error_client.py`, `clients/python/monlight/metrics_client.py`, `clients/python/monlight/integrations/__init__.py`, `clients/python/monlight/integrations/fastapi.py`, `clients/python/tests/test_scaffolding.py`
- Also struck through all shared infrastructure tasks in plan.md (implemented inline in services)
- **Learnings for future iterations:**
  - `setuptools.backends._legacy:_Backend` requires very recent setuptools (>= 72); use `setuptools.build_meta` for compatibility
  - The system pip has `PIP_REQUIRE_VIRTUALENV=1` set; always create a venv first with `python3 -m venv .venv`
  - Python 3.11 is available at system level via pyenv
  - The `.venv/` directory is already in `.gitignore`
  - `pytest-asyncio` with `asyncio_mode = "auto"` in pyproject.toml avoids needing `@pytest.mark.asyncio` on every async test
---

## Session 16: Python Error Client Tests & Verification

- What was implemented: Verified existing ErrorClient implementation and wrote 31 comprehensive unit tests covering payload formatting, PII filtering, HTTP behavior, fire-and-forget error handling, and sync variant
- Files changed: `clients/python/tests/test_error_client.py` (new), `plan.md` (struck through error client tasks)
- **Learnings for future iterations:**
  - `pytest-httpx` provides `httpx_mock` fixture that intercepts both async and sync httpx calls — use `httpx_mock.add_response()` for success cases, `httpx_mock.add_exception()` for failure simulation
  - `httpx_mock.get_requests()` returns list of captured requests; headers are lowercased in the request object (e.g., `requests[0].headers["x-api-key"]`)
  - `caplog` fixture with `caplog.at_level(logging.WARNING, logger="monlight.error_client")` captures log records for assertion
  - The ErrorClient was already fully implemented in session 15 as part of scaffolding — the plan tasks just needed test verification and strikethrough
---

## Session 17: Python MetricsClient Tests & Verification

- What was implemented: Verified existing MetricsClient implementation meets all acceptance criteria and wrote 42 comprehensive unit tests covering buffering (counter/histogram/gauge), flush behavior, connection failure handling, periodic timer, shutdown, configuration, and thread safety
- Files changed: `clients/python/tests/test_metrics_client.py` (new), `plan.md` (struck through metrics client tasks)
- **Learnings for future iterations:**
  - MetricsClient was already fully implemented in session 15 as part of scaffolding — only needed test verification and strikethrough
  - `MetricsClient.flush()` clears the buffer before sending (copy + clear under lock), so metrics are dropped on failure — this is intentional (no retry semantics)
  - For testing the periodic timer, use a very short `flush_interval` (0.1s) and `time.sleep(0.5)` to verify it fires — keeps tests fast
  - `client.start()` must always be paired with `client.shutdown()` in tests (use try/finally) to avoid dangling timer threads
  - The MetricsClient `_buffer` list and `_lock` are easily inspectable in tests since they're instance attributes, making buffer state assertions straightforward
---

## Session 18: FastAPI MonlightExceptionHandler Verification

- What was implemented: Verified that the existing `MonlightExceptionHandler` (implemented in session 15) meets all 4 acceptance criteria: (1) catches unhandled exceptions excluding HTTPException/RequestValidationError (via FastAPI's handler hierarchy), (2) calls `ErrorClient.report_error()` with request and exception via fire-and-forget `asyncio.create_task`, (3) logs error locally via `logger.exception()` and returns 500 JSON response, (4) installable via `app.add_exception_handler(Exception, handler)`.
- Files changed: `plan.md` (struck through MonlightExceptionHandler task)
- **Learnings for future iterations:**
  - The FastAPI integration code was fully implemented in session 15 as part of scaffolding — the remaining plan tasks just need verification and strikethrough
  - `MonlightExceptionHandler` is an async function (not a class) — it's registered directly as `app.add_exception_handler(Exception, MonlightExceptionHandler)`
  - HTTPException and RequestValidationError exclusion works automatically because FastAPI registers its own handlers for those types, which take precedence over the generic `Exception` handler
  - The error client is stored at `request.app.state.monlight_error_client` — this is the convention used by `setup_monlight()` to wire things up
---

## Session 19: MonlightMiddleware Verification & Tests

- What was implemented: Verified existing `MonlightMiddleware` (ASGI middleware for request metrics) meets all 6 acceptance criteria and wrote 25 comprehensive tests covering metric emission (counter + histogram), label correctness (method, endpoint, status), endpoint normalization with parameterized FastAPI routes, timing accuracy, installation via `add_middleware`, and edge cases (HTTP errors, unknown paths).
- Files changed: `clients/python/tests/test_middleware.py` (new), `plan.md` (struck through MonlightMiddleware task)
- **Learnings for future iterations:**
  - MonlightMiddleware was fully implemented in session 15 as part of scaffolding — only needed test verification and strikethrough
  - `MonlightMiddleware` extends `BaseHTTPMiddleware` from Starlette — test it with `fastapi.testclient.TestClient` and a mock `MetricsClient` (using `MagicMock(spec=MetricsClient)`)
  - The middleware calls `self.metrics_client.counter("http_requests_total", labels={...})` and `self.metrics_client.histogram("http_request_duration_seconds", value=duration, labels={...})` — labels are always passed as kwargs
  - Endpoint normalization relies on `request.scope.get("route")` which is populated by FastAPI's router — for unknown paths (404s), this is None and the raw URL path is used instead
  - Status code is always stored as a string (e.g., `"200"` not `200`) in the labels dict
---

## Session 20: setup_monlight() Verification & Tests

- What was implemented: Verified existing `setup_monlight()` function (implemented in session 15) meets acceptance criteria and wrote 16 comprehensive tests covering full setup (both URLs), error tracking only, metrics only, no-op (no URLs), default parameter values, client configuration verification, and end-to-end behavior.
- Files changed: `clients/python/tests/test_setup_monlight.py` (new), `plan.md` (struck through setup_monlight task)
- **Learnings for future iterations:**
  - `setup_monlight()` was already fully implemented in session 15 as part of scaffolding — only needed test verification and strikethrough
  - When testing with `httpx_mock` and MetricsClient that flushes on shutdown, you need to mock the metrics POST endpoint too (otherwise teardown fails with "unexpected request")
  - Use `@pytest.mark.httpx_mock(can_send_already_matched_responses=True)` when a test may trigger the same mock multiple times (e.g., error report + metrics flush both need HTTP mocks)
  - `patch.object(MetricsClient, "start")` effectively prevents the timer from running in tests while still verifying `start()` was called
---

## Session 21: CI/CD Workflow for Zig Services

- What was implemented: Created GitHub Actions workflow (`.github/workflows/zig-services.yml`) for all three Zig services with test, Docker build, image size verification, and GHCR push stages. Also fixed a known test issue across all three services where the `config.test.load fails when API_KEY is missing` test was not environment-aware.
- Files changed:
  - `.github/workflows/zig-services.yml` (new) — CI/CD workflow with matrix strategy for 3 services
  - `error-tracker/src/config.zig` — Made config test environment-aware
  - `log-viewer/src/config.zig` — Made config test environment-aware
  - `metrics-collector/src/config.zig` — Made config test environment-aware
  - `deploy/docker-compose.monitoring.yml` — Fixed build context for log-viewer and metrics-collector (was `build: ../log-viewer`, now uses `context: ..` + `dockerfile:` to ensure `shared/` directory is accessible)
  - `plan.md` — Struck through CI/CD Zig workflow task and Python client test tasks
- **Learnings for future iterations:**
  - The config tests that check "API_KEY is missing" must be environment-aware since CI sets `API_KEY` for other tests. Solution: check `std.posix.getenv("API_KEY")` and verify the appropriate behavior (success vs error).
  - `goto-bus-stop/setup-zig@v2` is the recommended GitHub Action for installing Zig in CI workflows.
  - Docker builds for all three services require `context: .` (repo root) because they `COPY shared/` — using shorthand `build: ../service` only sets context to the service directory and won't find `shared/`.
  - The workflow uses `docker/build-push-action@v5` with GHA cache (`cache-from: type=gha`, `cache-to: type=gha,mode=max`) for efficient Docker layer caching.
  - Image size check builds the image locally with `docker build -t size-check` since the build-push action may not leave images locally on PRs.
  - The `hashFiles` function in GitHub Actions cache keys supports glob patterns for cache invalidation on source changes.
---

## Session 22: CI/CD Workflow for Python Client

- What was implemented: Created GitHub Actions workflow (`.github/workflows/python-client.yml`) for the Python client package with test matrix (Python 3.10, 3.11, 3.12), path-filtered triggers, and optional PyPI publishing on tagged releases.
- Files changed:
  - `.github/workflows/python-client.yml` (new) — CI/CD workflow with Python version matrix and PyPI publish job
  - `plan.md` — Struck through Python client CI/CD task
- **Learnings for future iterations:**
  - Path filtering in GitHub Actions (`paths: ['clients/python/**']`) ensures the workflow only runs when Python client files change, saving CI resources.
  - `pypa/gh-action-pypi-publish@release/v1` with `id-token: write` permission uses trusted publishing (OIDC) — no PyPI API token secret needed if the PyPI project is configured for GitHub Actions trusted publishing.
  - Tag convention `python-v*` (e.g., `python-v0.1.0`) distinguishes Python client releases from potential Zig service releases, allowing independent versioning.
  - Testing across Python 3.10/3.11/3.12 ensures compatibility with the `requires-python = ">=3.10"` constraint in pyproject.toml.
  - All 121 Python client tests pass locally — the test job in CI should produce the same results.
---

## Session 23: End-to-End Validation

- What was implemented: Created `deploy/docker-compose.test.yml` and `deploy/smoke-test.sh` for end-to-end smoke testing of all three Monlight services. The compose file uses isolated ports (15010-15012) and a dedicated `monlight_e2e_test` network (not the production `flowrent_network`). The smoke test script covers all 6 acceptance criteria: health checks, error ingestion (create + duplicate), log viewer with empty log directory, metrics ingestion, web UI accessibility, and graceful shutdown via SIGTERM.
- Files changed:
  - `deploy/docker-compose.test.yml` (new) — test compose configuration with isolated ports, volumes, and network
  - `deploy/smoke-test.sh` (new) — comprehensive smoke test script with pass/fail tracking
  - `plan.md` — struck through end-to-end validation task
- **Learnings for future iterations:**
  - Docker daemon is not available in this development environment — e2e tests must be run manually on a machine with Docker
  - The `version` field in docker-compose files is obsolete and produces warnings; omit it for cleaner output
  - For e2e tests, use high port numbers (15xxx) to avoid conflicts with production services (5xxx)
  - Log Viewer gracefully handles empty/missing LOG_SOURCES — it retries every poll interval without crashing
  - The `docker compose config --quiet` command validates compose file syntax without requiring a running daemon
  - Use `docker kill --signal=SIGTERM` (not `docker stop`) to test graceful shutdown behavior, since `docker stop` sends SIGTERM then waits before SIGKILL
  - All three services implement `--healthcheck` CLI flag that connects to localhost:8000 and checks for 200 response
---

## Session 24: Backup Script (deploy/backup.sh)

- What was implemented: Verified and committed the existing `deploy/backup.sh` script which backs up all three SQLite databases (errors.db, logs.db, metrics.db) using SQLite's `.backup` command. The script supports selective backup (by name), retains the last 7 daily backups with automatic pruning, verifies backup integrity via `PRAGMA integrity_check`, and includes a commented-out S3 upload section with detailed setup instructions.
- Files changed: `deploy/backup.sh` (committed as new file), `plan.md` (struck through backup script task)
- **Learnings for future iterations:**
  - The backup script was already created in a previous session but not committed — always check `git status` for uncommitted files before implementing
  - SQLite `.backup` command is the correct way to back up WAL-mode databases — it creates a consistent snapshot even while the database is in use
  - The `*.db` pattern in `.gitignore` already covers backup .db files, so the `deploy/backups/` directory contents are automatically ignored
   - `bash -n script.sh` validates bash syntax without executing the script — useful as a quick quality check for shell scripts
---

## Session 25: Upgrade Script (deploy/upgrade.sh)

- What was implemented: Created `deploy/upgrade.sh` — an upgrade script that pulls latest code, rebuilds Docker images, and restarts Monlight services one at a time with health verification after each restart. Includes pre-upgrade backup, rollback image tagging, and detailed rollback instructions on failure.
- Files changed: `deploy/upgrade.sh` (new), `plan.md` (struck through upgrade script task)
- **Learnings for future iterations:**
  - The upgrade script follows the same style/conventions as `backup.sh`: `set -euo pipefail`, `log()/warn()/die()` helpers, `SCRIPT_DIR` pattern, associative arrays for service config
  - `docker compose up -d --no-deps <service>` recreates only the specified service without restarting its dependencies
  - `git pull --ff-only` is safer than `git pull` for automated scripts — it fails on merge conflicts rather than opening an editor
  - Pre-upgrade image tagging (`docker tag <image>:latest <image>:rollback`) provides a fast rollback path without needing to rebuild
  - Health checks poll `/health` on each service's external port (5010, 5011, 5012) — these endpoints require no API key
---

## Session 26: Verify Deployment Config Completeness

- What was implemented: Updated `deploy/docker-compose.monitoring.yml` to fix three deployment config gaps: (1) mapped service-specific API keys from `secrets.env` (e.g., `LOG_VIEWER_API_KEY`) to the `API_KEY` env var each service reads, (2) added `LOG_LEVEL` env var with `${LOG_LEVEL:-info}` default to all three services, (3) added `deploy.resources.limits.memory: 30M` to all three services. Also removed obsolete `version: "3.8"` field and documented `LOG_LEVEL` in `secrets.env.example`.
- Files changed: `deploy/docker-compose.monitoring.yml`, `deploy/secrets.env.example`, `plan.md`
- **Learnings for future iterations:**
  - All three Zig services read `API_KEY` (not prefixed names) from their environment, but `secrets.env` uses prefixed names (`ERROR_TRACKER_API_KEY`, `LOG_VIEWER_API_KEY`, `METRICS_COLLECTOR_API_KEY`). The docker-compose `environment` section maps them: `API_KEY=${LOG_VIEWER_API_KEY}`
  - For Docker Compose v2 (non-Swarm), use `deploy.resources.limits.memory` for memory limits — this works with `docker compose` CLI even without Swarm mode
  - The `${VAR:-default}` syntax in docker-compose environment values allows providing defaults while still supporting override from secrets.env or host environment
  - The obsolete `version: "3.8"` field produces warnings in modern Docker Compose — omit it
---

## Session 27: Browser Relay Bootstrap (build.zig + build.zig.zon)

- What was implemented: Created the `browser-relay/` directory with `build.zig`, `build.zig.zon`, and a minimal `src/main.zig` stub. The build system imports all four shared modules (sqlite, config, auth, rate_limit), links SQLite C library, and produces a working `browser-relay` binary. The main.zig includes a `--healthcheck` CLI flag (raw TCP, same pattern as other services) and a placeholder test.
- Files changed: `browser-relay/build.zig` (new), `browser-relay/build.zig.zon` (new), `browser-relay/src/main.zig` (new), `plan.md` (struck through build.zig task)
- **Learnings for future iterations:**
  - The browser-relay build.zig follows the exact same pattern as error-tracker: shared modules via `b.addModule` + `addImport`, `linkSystemLibrary("sqlite3")` + `linkLibC()` on both exe and test artifacts
  - Healthcheck uses raw TCP (same as all other services) — Zig 0.13's `std.http.Client.Request` struct does NOT have a `.status` field accessible after `wait()`; the raw TCP approach is simpler and proven
  - LSP diagnostics on `.zig.zon` files show "expected enum literal" errors — these are false positives from the language server, not actual build issues
  - Binary size for the minimal stub is ~2.3MB (debug, not stripped) — similar to other services at this stage
---

## Session 27b: Browser Relay Dockerfile & HTTP Server Skeleton

- What was implemented: Created the browser-relay Dockerfile (multi-stage Alpine build, same pattern as other services) and expanded `src/main.zig` to a full HTTP server skeleton: binds to 0.0.0.0:8000, accept loop with `handleConnection`, `/health` endpoint returning `{"status": "ok"}`, 404 handler, `sendJsonResponse` helper, and `--healthcheck` CLI flag. The `handleConnection` is `pub` so future test files can import it.
- Files changed: `browser-relay/Dockerfile` (new), `browser-relay/src/main.zig` (expanded), `plan.md` (struck through Dockerfile and HTTP server skeleton tasks)
- **Learnings for future iterations:**
  - The browser-relay Dockerfile is identical to error-tracker's Dockerfile pattern: `alpine:3.21` builder with `zig sqlite-dev musl-dev`, copies service dir + shared/, builds with `-Doptimize=ReleaseSafe`, runtime stage with only `sqlite-libs`
  - Port 8000 may already be in use in development — `AddressInUse` errors are expected when testing locally if another service is running
  - The `max_body_size` for browser-relay is 64KB (vs 256KB for error-tracker) — browser payloads are smaller
  - The HTTP server skeleton is intentionally minimal: no config, database, auth, or rate limiting yet — those will be added incrementally by subsequent plan tasks
---

## Session 27c: Browser Relay Config, Database & Integration

- What was implemented: Created `config.zig` (loads 10 env vars including 5 required: `ADMIN_API_KEY`, `ERROR_TRACKER_URL`, `ERROR_TRACKER_API_KEY`, `METRICS_COLLECTOR_URL`, `METRICS_COLLECTOR_API_KEY`), `database.zig` (3 migrations: `dsn_keys` table, `source_maps` table, indexes), and integrated both into `main.zig` with auth middleware, rate limiting, and body size enforcement.
- Files changed: `browser-relay/src/config.zig` (new), `browser-relay/src/database.zig` (new), `browser-relay/src/main.zig` (expanded with config/db/auth/rate-limit integration), `browser-relay/build.zig` (added config and database test targets), `plan.md` (struck through SQLite database layer and Configuration module tasks)
- **Learnings for future iterations:**
  - The shared sqlite module's `exec()` calls `log.err()` on failure, and Zig's test runner treats `log.err` calls as test failures ("logged errors"). To test unique constraint violations without triggering test failures, use `INSERT OR IGNORE` + `SELECT COUNT(*)` instead of `expectError(error.ExecFailed, ...)`.
  - SQLite's UNIQUE constraint on a column automatically creates an implicit unique index (`sqlite_autoindex_*`). The query planner may prefer this over an explicit index on the same column. Tests should check for `USING INDEX` generically rather than a specific index name.
  - Browser relay uses `ADMIN_API_KEY` (not `API_KEY`) for management endpoints — distinct from the service API key pattern used by the other three services. DSN public keys are a separate auth mechanism for browser-side ingestion.
  - The `@intCast` is needed when converting the `i64` return from `config_mod.getInt()` to `usize` for `max_body_size` and `rate_limit`.
---

## Session 28: Browser Relay DSN Auth & Admin Auth

- What was implemented: Created `dsn_auth.zig` module for DSN public key authentication (X-Monlight-Key header → dsn_keys table lookup → project name resolution), `dsn_auth_test.zig` with 14 integration tests covering auth routing and separation, and updated `main.zig` with path-based auth dispatch: `/health` (no auth), `/api/browser/*` (DSN auth), `/api/source-maps*` and `/api/dsn-keys*` (admin API key auth), unknown paths (404, no auth).
- Files changed: `browser-relay/src/dsn_auth.zig` (new), `browser-relay/src/dsn_auth_test.zig` (new), `browser-relay/src/main.zig` (modified), `browser-relay/build.zig` (modified), `plan.md` (struck through DSN auth tasks)
- **Learnings for future iterations:**
  - SQLite `row.text()` returns a pointer to internal statement memory that becomes invalid after `stmt.deinit()`. Always copy the string data into a local buffer (e.g., `[100]u8`) before finalizing the statement.
  - The `lookupProject()` helper uses a `[100]u8` buffer and returns a `?[]const u8` slice into that buffer — the caller must use it before the buffer goes out of scope.
  - `DsnAuthResult` struct packs both `authenticated: bool` and the project name buffer together, so the caller gets a self-contained result without dangling pointer concerns.
  - Path-based auth dispatch in `main.zig` strips query parameters before matching (`std.mem.indexOfScalar(u8, target, '?')`) to ensure `/api/browser/errors?foo=bar` routes correctly.
  - Integration tests use `TestServer` pattern with OS-assigned port (port 0) — same pattern as auth_test.zig and rate_limit_test.zig in other services.
   - The browser-relay uses `ADMIN_API_KEY` (from config) for admin endpoints, distinct from the `API_KEY` used by the other three services. The config module reads `ADMIN_API_KEY` env var.
---

## Session 29: Browser Relay CORS Middleware

- What was implemented: Created `cors.zig` module with CORS middleware for browser ingestion endpoints, `cors_test.zig` with 10 integration tests covering preflight OPTIONS, normal POST CORS headers, disallowed/missing origins, no-CORS-configured scenario, and CORS-only-on-browser-paths behavior. Integrated CORS into `main.zig` with path-based dispatch: OPTIONS preflight returns 204 before auth, allowed origins get `Access-Control-Allow-Origin` on responses, non-matching origins silently continue without CORS headers.
- Files changed: `browser-relay/src/cors.zig` (new), `browser-relay/src/cors_test.zig` (new), `browser-relay/src/main.zig` (modified — added cors import, handleCors dispatch, sendJsonResponseWithCors), `browser-relay/src/dsn_auth_test.zig` (modified — fixed use-after-free bug in HttpResponse), `browser-relay/build.zig` (modified — added cors test targets), `plan.md` (struck through CORS handling task)
- **Learnings for future iterations:**
  - **Use-after-free in test helpers**: `sendRequest()` returning `HttpResponse` with `raw: []const u8` slices pointing into a stack-local `response_buf` is undefined behavior — the buffer is freed when `sendRequest` returns. Fix: embed the `_buf: [4096]u8` inside the HttpResponse struct with `total_read`/`body_offset` fields, and use methods `raw()` / `body()` to derive slices from the owned buffer. This bug existed in `dsn_auth_test.zig` too but was masked because those tests mostly checked `status_code` (a value type).
  - **Header parsing with `": "` vs `:`**: When parsing HTTP headers from raw response bytes, always split on `": "` (colon-space), not just `:`. Headers like `access-control-allow-origin` contain hyphens (not colons), but `indexOfScalar(u8, line, ':')` finds the first colon in the VALUE portion (e.g., `https:`) and splits incorrectly.
  - **CorsConfig uses inline arrays** (`[max_origins][max_origin_len]u8`) to avoid heap allocation — origins are parsed from a comma-separated env var string at startup. Max 32 origins, each up to 256 bytes.
  - **CORS preflight vs normal flow**: OPTIONS preflight returns 204 with all CORS headers BEFORE DSN auth is checked (browsers send preflight without credentials). Normal requests check CORS after auth, and include `Access-Control-Allow-Origin` in the response if the origin is allowed.
  - **`getCorsHeaders()` is separate from `handleCors()`**: `handleCors` handles the OPTIONS preflight case (sends 204), while `getCorsHeaders` is used by normal response paths to get the CORS headers to include. This separation avoids duplicating origin-matching logic.
  - **Zig 0.13 `std.http.Server.Request.respond()` extra_headers**: The `extra_headers` field accepts `&.{ .{ .name = "...", .value = "..." } }` — an array of anonymous struct literals. Multiple headers are supported.
---

## Session 30: Source Map Deobfuscation Integration

- What was implemented: Integrated the existing `sourcemap.deobfuscateStackTrace()` engine into the browser error ingestion pipeline. When a browser error report includes a `release` field, the handler now attempts to deobfuscate the stack trace using source maps stored in the database before forwarding to the error tracker. If deobfuscation fails or no source maps are found, the raw stack trace is used as-is (graceful fallback).
- Files changed: `browser-relay/src/browser_errors.zig` (added `sourcemap.zig` and `sqlite` imports, added `db` parameter to both `handleBrowserError` and `handleBrowserErrorWithCors`, added deobfuscation logic between parse/validate and buildPayload), `browser-relay/src/main.zig` (pass `db` to `handleBrowserErrorWithCors` call), `browser-relay/build.zig` (added `sqlite_mod` import, `linkSystemLibrary("sqlite3")`, `linkLibC()` to `browser_errors_tests` target), `plan.md` (struck through source map deobfuscation integration sub-task lines 56-61)
- **Learnings for future iterations:**
  - **Mutable copy of value type struct**: `BrowserErrorFields` is returned by value from `parseAndValidate`. To replace `.stack` with a deobfuscated version, we create `var final_fields = fields;` and mutate `final_fields.stack`. This avoids changing the `parseAndValidate` API or adding a separate parameter to `buildPayload`.
  - **Arena allocator for deobfuscation**: The deobfuscated stack trace is allocated in a dedicated `deobfuscation_arena` with `defer deinit()`. This ensures the memory lives long enough to be written into the payload buffer by `buildPayload`, then is freed when the function returns.
  - **Error handling with labeled blocks**: The `catch |err| blk: { log.warn(...); break :blk null; }` pattern allows graceful fallback — if `deobfuscateStackTrace` returns an error, we log a warning and fall back to the raw stack trace by setting `deobfuscated_stack = null`.
  - **Build dependency propagation**: When a standalone test target (`browser_errors_tests`) starts importing a module that transitively imports `sqlite`, the build target needs `addImport("sqlite", sqlite_mod)` + `linkSystemLibrary("sqlite3")` + `linkLibC()`, even if the tests themselves don't directly use sqlite.
---

## Session 31: Verify & Strike Through Already-Implemented Plan Items

- What was implemented: Verified that the source map deobfuscation module (JS stack trace parser, VLQ decoder, stack trace rewriting), health check endpoint, and related test plan items were already fully implemented in sourcemap.zig but not struck through in plan.md. Struck through all verified items.
- Files changed: `plan.md` (struck through: source map deobfuscation module + 3 sub-tasks, health check endpoint, source map parsing tests, stack trace parsing tests, stack trace rewriting tests)
- **Learnings for future iterations:**
  - The sourcemap.zig module (1064 lines) contains the full deobfuscation pipeline: Base64 VLQ decoder, source map JSON parser, JS stack trace parser (Chrome/Firefox/Safari), stack trace rewriter, and database-backed source map cache — all with 22 unit tests
  - When previous sessions implement features but forget to strike through plan items, subsequent sessions should verify and clean up the plan rather than re-implementing
  - The browser-relay health check endpoint (`/health`) is handled before auth middleware in main.zig, so it works without any API key
---

## Session 32: Browser Relay Data Retention Cleanup (Integration)

- What was implemented: Completed integration of the `retention.zig` module (created in session 31) into the browser-relay service. Added the retention test target to `build.zig` (with sqlite module, linkSystemLibrary, linkLibC), integrated the retention background thread into `main.zig` following the exact pattern from error-tracker (std.atomic.Value(bool) stop flag, std.Thread.spawn, defer stop+join), and verified all 13 test suites (27/27 build steps) pass including the 6 retention unit tests.
- Files changed: `browser-relay/build.zig` (added retention_tests target + dependOn), `browser-relay/src/main.zig` (added retention import, retention_cleanup_interval_ns constant, retention thread spawn/defer), `plan.md` (struck through data retention cleanup tasks)
- **Learnings for future iterations:**
  - The retention thread integration pattern is identical across services: import module, define interval constant, create atomic stop flag, spawn thread with (dbPathZ, retention_days, interval_ns, &stop), defer { stop.store(true, .release); thread.join(); }
  - `retention.zig` imports `database.zig` for schema initialization in tests (`:memory:` databases need migration before inserting test data) — this is a file-level import, not a build.zig module import
  - `zig build test --summary all` shows the count of test suites (13 for browser-relay) and whether they ran or were cached

## Session 33: Upstream Forwarding Failure Tests (browser-relay)

- What was implemented: Created `browser-relay/src/forward_test.zig` with 5 integration tests verifying that the browser-relay returns proper 502 responses when upstream services (Error Tracker, Metrics Collector) are down. Tests cover: error forwarding failure (502 status + JSON body), metrics forwarding failure (502 status + JSON body), and graceful degradation with multiple sequential requests to both endpoints. Added `test-forward` build step to `browser-relay/build.zig` for isolated testing.
- Files changed: `browser-relay/src/forward_test.zig` (new, 244 lines — 5 integration tests), `browser-relay/build.zig` (added forward_tests target + test-forward step + dependOn), `plan.md` (struck through upstream forwarding failure test tasks)
- **Learnings for future iterations:**
  - The browser metrics endpoint expects a JSON object `{"metrics":[...]}` with a `metrics` array, NOT a bare JSON array `[...]`. Each metric must have `name` (string), `type` (counter/histogram/gauge), and `value` (number). Optional top-level fields: `session_id`, `url`.
  - `std.http.Client` in Zig 0.13.0 returns `error.ConnectionRefused` instantly when connecting to a non-listening port — no TCP timeout delay. This makes testing upstream failures fast.
  - The Zig build cache (`zig-cache/`) can serve stale test binaries after source edits. If tests show unexpected results, delete `.zig-cache/` and rebuild.
  - Test config points upstream URLs to `http://127.0.0.1:19999` and `http://127.0.0.1:19998` (deliberately not running) to trigger connection-refused errors in the forwarding functions.
  - Browser-relay now has 14 test suites (29 build steps, 460 tests total) — the forward_test adds 5 dedicated tests plus 98 transitive tests from imported modules.

## Session 34: Error Tracker Browser Error Support (Backend + UI)

- What was implemented: Completed the Error Tracker modifications for browser error support. This includes:
  1. **JS stack trace fingerprinting** (`fingerprint.zig`): Extended `extractLastLocation()` to handle Chrome/V8 (`at func (file:line:col)`), Firefox/Safari (`func@file:line:col`), and anonymous (`@file:line:col`) JS stack formats. Python format tried first (returns last match), JS format second (returns first match — throw site). Column number is stripped so same-location-different-column errors group together. Added `extractJsLocation()`, `findJsFrameAt()`, `parseFileLineCol()`, `allDigits()` helpers. 11 new tests (23 total).
  2. **Source filter for error listing API** (`error_listing.zig`): Added `source` field to `ListParams` (accepts "browser" or "server"). Refactored query construction from combinatorial if/else branches to dynamic SQL via `buildWhereClause()` using a stack buffer (runtime string construction with `@memcpy`, null-terminated for SQLite). Source filter uses subquery: `AND id IN (SELECT error_id FROM error_occurrences WHERE request_method = 'BROWSER')`. Added `bindFilterParams()` and `whereSlice()` helpers. 7 new tests (30 total for error_listing).
  3. **Web UI — error listing** (`index.html`): Added "Source" dropdown filter (All/Server/Browser), wired to `loadErrors()` as `source` query parameter.
  4. **Web UI — error detail** (`error_detail.html`): Added purple color for `BROWSER` method in `methodColor()`. Added `renderBrowserContext()` function that prominently displays `page_url`, `user_agent`, and `session_id` from the `extra` field in a styled purple-tinted info bar above the occurrence details.
  5. **Browser ingestion tests** (`error_ingestion.zig`): 3 new tests — BROWSER method accepted and stored, JS fingerprint grouping (same file:line = same error), `parseAndValidate` accepts BROWSER with extra context (uses ArenaAllocator to avoid leaks from `parseFromSliceLeaky`).
- Files changed: `error-tracker/src/fingerprint.zig` (JS stack trace support), `error-tracker/src/error_listing.zig` (source filter + dynamic SQL refactor), `error-tracker/src/error_ingestion.zig` (browser ingestion tests), `error-tracker/src/static/index.html` (source filter dropdown), `error-tracker/src/static/error_detail.html` (BROWSER method color + browser context display), `plan.md` (struck through Error Tracker modification tasks)
- Test results: error-tracker 35/35 steps, 393 tests passed; all other services pass (log-viewer 17/17, metrics-collector 17/17, browser-relay 29/29, Python client 121 tests)
- **Learnings for future iterations:**
  - Zig's `++` operator is comptime-only string concatenation. You cannot use it on runtime `[]const u8` slices. For dynamic SQL construction, use `@memcpy` into a stack buffer and create a sentinel-terminated slice with `buf[0..len :0]`.
  - `db.prepare()` requires `[*:0]const u8` (null-terminated). When building SQL at runtime, the buffer must have a null byte at `buf[total_len] = 0` and be cast via `sql_buf[0..total_len :0]`.
  - `sqlite.Statement.bindInt/bindText` take `self: Statement` by value (not pointer). Pass `stmt` not `&stmt`.
  - `std.json.parseFromSliceLeaky` allocates into the provided allocator and never frees. When testing with `std.testing.allocator` (which is a GPA that detects leaks), wrap with `std.heap.ArenaAllocator` and defer `arena.deinit()`.
  - The source filter SQL uses a subquery rather than a JOIN to avoid row duplication when an error has multiple occurrences with different request_method values.
  - Session ID grouping (linking to other errors from same session) is deferred — currently just displayed inline. **UPDATE: implemented in next session.**

## Session 7 — Session ID grouping filter
- **Task:** Show session ID as a grouping hint (link to other errors from same session)
- **Implementation:**
  1. **Backend — session_id filter** (`error_listing.zig`): Added `session_id` field to `ListParams`, parsed from query params. Added WHERE clause using `json_extract(extra, '$.session_id') = ?` subquery on `error_occurrences` table (same pattern as source filter). Updated `bindFilterParams()` to bind session_id after project and environment. Added `insertTestOccurrenceWithExtra()` test helper. 5 new tests: parse session_id, ignore empty, filter returns matching errors, multiple errors from same session, no matches returns empty.
  2. **Error detail UI** (`error_detail.html`): Changed session ID display from static `<span>` to clickable `<a>` link pointing to `/?session_id=<value>`. Styled with dotted underline and hover color, with descriptive title attribute.
  3. **Error listing UI** (`index.html`): Added amber session filter banner (hidden by default) that appears when `session_id` URL param is present. Shows "Showing errors from session: <id>" with a "Clear filter" button. `loadErrors()` passes session_id to API. `initSessionBanner()` called on startup.
- Files changed: `error-tracker/src/error_listing.zig`, `error-tracker/src/static/error_detail.html`, `error-tracker/src/static/index.html`, `plan.md`
- Test results: 35/35 steps, 246/246 tests passed
- **Learnings for future iterations:**
   - SQLite's `json_extract()` works well for filtering on fields inside JSON columns — same approach used for labels in metrics-collector.
   - The subquery pattern `id IN (SELECT error_id FROM error_occurrences WHERE ...)` keeps the main query clean and avoids JOIN row duplication.
   - `encodeURIComponent()` in JS is important when passing session IDs as URL parameters since they may contain special characters.

## Session 35: Metrics Collector — Web Vitals Dashboard Extension

- What was implemented: Extended the metrics-collector dashboard to show Web Vitals data when browser metrics exist. This includes:
  1. **Backend — `buildDashboardJson()` in `main.zig`**: Added conditional `web_vitals` section to the dashboard JSON response. First queries `metrics_raw` for `web_vitals_lcp/inp/cls` metrics with `json_extract(labels, '$.source') = 'browser'`. If data exists, emits three sub-sections:
     - `summary`: per-vital AVG value, count, and rating (good/needs-improvement/poor based on Google thresholds: LCP <2500/<4000, INP <200/<500, CLS <0.1/<0.25)
     - `timeseries`: time-bucketed averages for all three vitals (minute resolution for 1h/24h, hour resolution for 7d/30d)
     - `by_page`: per-page breakdown with LCP/INP/CLS averages and page view counts, sorted by views descending, limited to 20 pages
  2. **Frontend — `index.html`**: Added Web Vitals UI section (hidden when no data):
     - Three summary cards showing LCP (ms), INP (ms), CLS with color-coded values and pill ratings
     - uPlot trend chart showing LCP, INP, CLS over time (CLS scaled x1000 for visibility alongside ms-scale metrics)
     - Page performance breakdown table with color-coded values per Google thresholds
  3. **Tests**: 3 new tests in `main.zig`:
     - `buildDashboardJson without web vitals data` — verifies `web_vitals` key is absent
     - `buildDashboardJson with web vitals data` — verifies full section with summary/timeseries/by_page
     - `buildDashboardJson web vitals ratings are correct` — verifies poor LCP (5000ms), needs-improvement INP (350ms), good CLS (0.05)
- Files changed: `metrics-collector/src/main.zig` (Web Vitals queries + `writeWebVitalField` helper + 3 tests), `metrics-collector/src/static/index.html` (Web Vitals UI section + CSS + JS rendering functions), `plan.md` (struck through Metrics Collector Web Vitals tasks), `progress.txt`
- Test results: 17/17 build steps, 27/27 tests passed (was 4/4, now 26/26 in main test suite + 1 in web_ui)
- **Learnings for future iterations:**
  - SQLite's `json_extract(labels, '$.source')` works directly on the TEXT column containing JSON — no need to parse/index JSON separately
  - The `metrics_aggregated` table has p50/p95/p99 but NOT p75. For Web Vitals timeseries, we use AVG from `metrics_raw` as a practical approximation. True p75 would require either adding a p75 column to aggregation or computing at query time with NTILE window functions.
  - Conditional aggregation with `CASE WHEN name='web_vitals_lcp' THEN value END` allows querying all three vitals in a single SQL query per time bucket, avoiding 3x query overhead.
  - CLS values (typically 0.01-0.25) are ~10000x smaller than LCP values (typically 1000-5000ms). The frontend scales CLS by 1000 for the trend chart to make all three series visible on the same y-axis.
  - The `writeWebVitalField` helper writes nullable float values with 4 decimal places — same pattern as `writeNullableFloat` in `metrics_query.zig` but with `.4` precision instead of `.6`.

## Session 36: JavaScript SDK Package Scaffolding

- What was implemented: Created the `clients/js/` directory with complete package scaffolding for the `@monlight/browser` SDK. This includes package.json (with esbuild, TypeScript, vitest dev dependencies), tsconfig.json (ES2020 target, ESNext module), build.mjs (esbuild pipeline producing UMD + ESM bundles + TypeScript declarations), vitest.config.ts, and all 8 source modules with full TypeScript implementations.
- Files changed:
  - `clients/js/package.json` (new) — Package metadata, scripts, dependencies
  - `clients/js/package-lock.json` (new) — Locked dependency versions
  - `clients/js/tsconfig.json` (new) — TypeScript configuration
  - `clients/js/build.mjs` (new) — esbuild build script producing UMD, ESM, and .d.ts
  - `clients/js/vitest.config.ts` (new) — Test runner config with jsdom environment
  - `clients/js/src/types.ts` (new) — TypeScript interfaces: MonlightConfig, MonlightClient, BrowserError, BrowserMetric, ResolvedConfig
  - `clients/js/src/session.ts` (new) — Session ID generation (UUID v4, sessionStorage persistence)
  - `clients/js/src/transport.ts` (new) — Batched beacon transport with sendBeacon fallback
  - `clients/js/src/errors.ts` (new) — Error capture with deduplication, console patching, global handlers
  - `clients/js/src/metrics.ts` (new) — Metrics convenience wrapper (counter, histogram, gauge)
  - `clients/js/src/vitals.ts` (new) — Web Vitals collection (LCP, INP, CLS, FCP, TTFB, page load time)
  - `clients/js/src/network.ts` (new) — Fetch/XHR interception with URL sanitization
  - `clients/js/src/core.ts` (new) — init() function, MonlightClient implementation, UMD auto-init
  - `plan.md` — Struck through Package scaffolding tasks
- Build output:
  - `dist/monlight.min.js` — 13KB raw, 3.9KB gzipped (UMD/IIFE)
  - `dist/monlight.esm.js` — 13KB raw, 3.7KB gzipped (ESM)
  - `dist/core.d.ts` + supporting .d.ts — TypeScript declarations
- **Learnings for future iterations:**
  - Node.js 21.0.0 and npm 10.2.0 are available in the dev environment
  - `esbuild` produces very small bundles — 3.9KB gzipped for the full SDK with all modules
  - TypeScript `tsc --emitDeclarationOnly` generates one .d.ts per source file. For a single entry point, `package.json` `types` field points to `dist/core.d.ts` which re-exports the public types
  - The `noUnusedLocals` and `noUnusedParameters` tsconfig options catch dead code early — classes storing config for future use need to actually reference it
  - `vitest` with `jsdom` environment provides browser-like globals (window, document, etc.) for testing
  - The UMD bundle uses `format: "iife"` with `globalName: "MonlightSDK"` — the auto-init from `window.MonlightConfig` runs as part of module evaluation
   - `sendBeacon` cannot set custom headers, so the DSN key is passed as a query parameter `?key={dsn}` when using beacon transport
---

## Session 37: JS SDK Core Module Verification & Tests

- What was implemented: Verified that the `core.ts` module (init(), MonlightClient, UMD auto-init) fully meets all acceptance criteria from the plan. Wrote 38 comprehensive unit tests covering: config validation (missing/empty dsn and endpoint throw errors), successful initialization, default config values, trailing slash stripping, enabled:false no-op client, beforeSend callback, MonlightClient methods (captureError, captureMessage, setUser, addContext, destroy), destroy cleanup (event listeners, console restoration), UMD auto-initialization pattern, sub-module initialization verification (error/visibilitychange/pagehide listeners, console patching), and sampleRate 0 behavior.
- Files changed: `clients/js/src/core.test.ts` (new, 38 tests), `plan.md` (struck through Core module tasks)
- Test results: 38/38 tests pass, typecheck passes, build succeeds, bundle 3.9KB gzipped
- **Learnings for future iterations:**
  - vitest with jsdom provides `window`, `document`, `sessionStorage`, `console` globals automatically — no extra setup needed
  - The auto-init code in core.ts runs on module load, so re-importing in tests doesn't re-trigger it (module cache). Test the auto-init pattern by simulating what the code does rather than trying to re-import
  - `vi.spyOn(window, "addEventListener")` captures listener registrations and can be checked for specific event types (error, unhandledrejection, pagehide, visibilitychange)
  - The no-op client returned when `enabled: false` is a plain object with arrow functions, not a MonlightClientImpl instance — this means no sub-modules are initialized at all
  - Error deduplication uses a module-level `dedupCache` Map — tests that call `captureError` multiple times with the same error within 60s will have the second call deduplicated (returns null from buildPayload)
---

## Session 38: JS SDK Module Verification & Tests (session, errors, vitals, network, transport)

- What was implemented: Verified all remaining JS SDK source modules against plan acceptance criteria and wrote comprehensive unit tests for each. Also struck through type definitions, script tag integration, and npm package integration plan items (all already implemented).
- Files changed:
  - `clients/js/src/session.test.ts` (new, 15 tests) — UUID v4 format, sessionStorage persistence, new ID per tab, no cookies/localStorage, crypto.randomUUID fallback
  - `clients/js/src/errors.test.ts` (new, 38 tests) — global error/unhandledrejection handlers, manual captureError/captureMessage, setUser/addContext, deduplication (suppression + time expiry + cache clear), beforeSend (modify + drop), console.error/warn patching + restore
  - `clients/js/src/vitals.test.ts` (new, 26 tests) — LCP/INP/CLS tracking + reporting on page hide, FCP observation, TTFB/page load time from navigation timing, sampling (sampleRate 0/0.5/1.0), destroy disconnects observers
  - `clients/js/src/network.test.ts` (new, 22 tests) — fetch interception (counter+histogram metrics, 500+ error capture, network error, monitoring endpoint exclusion), XHR patching/restore, URL sanitization (query stripping, UUID/numeric ID collapsing)
  - `clients/js/src/transport.test.ts` (new, 25 tests) — sendError (immediate, headers, keepalive), bufferMetric (batching at 10), flushMetrics, periodic flush (5s interval), sendBeacon on pagehide (Blob, DSN as query param), destroy (timer stop, listener removal, final flush), error handling (graceful failure, debug logging)
  - `plan.md` — struck through: session module, error capture module, console capture module, vitals module, network module, transport module, type definitions, script tag integration, npm package integration, all test plan items
- Test results: 164/164 tests pass across 6 test files, typecheck clean, build successful
- **Learnings for future iterations:**
  - jsdom does not have `PromiseRejectionEvent` — use `new Event("unhandledrejection")` with `(event as any).reason = reason` to simulate promise rejections in tests
  - jsdom does not have `PerformanceObserver` — create a mock class that stores callbacks by entry type, then simulate entries by calling `callback({ getEntries: () => entries })`
  - `vi.useFakeTimers()` enables `vi.advanceTimersByTime()` for testing periodic flush intervals (5s) without real delays
  - `vi.spyOn(Math, "random").mockReturnValue(x)` controls the sampling decision in VitalsCollector constructor — use values above/below sampleRate to test inclusion/exclusion
  - `Object.defineProperty(document, "visibilityState", { value: "hidden", writable: true, configurable: true })` is needed to simulate page visibility changes in jsdom
  - `navigator.sendBeacon` is not available in jsdom by default — define it via `Object.defineProperty(navigator, "sendBeacon", ...)` and clean up after test
  - When mocking `window.fetch` for network interception tests, save the original before creating the NetworkMonitor (which patches it on `install()`), and restore in afterEach
   - The JS SDK's entire test suite (164 tests across 6 files) runs in ~1.7s — vitest with jsdom is fast
---

## Session 39: Deployment Updates for Browser Relay

- What was implemented: Updated all deployment configuration files to include the browser-relay service:
  1. **docker-compose.monitoring.yml**: Added `browser-relay` service — port 5013:8000, SQLite volume (`./data/browser-relay:/app/data`), 7 environment variables (DATABASE_PATH, ADMIN_API_KEY, ERROR_TRACKER_URL/API_KEY, METRICS_COLLECTOR_URL/API_KEY, CORS_ORIGINS, LOG_LEVEL), env_file, health check (`--healthcheck`), 30M memory limit, depends_on error-tracker and metrics-collector (condition: service_healthy), flowrent_network.
  2. **secrets.env.example**: Added `BROWSER_RELAY_ADMIN_API_KEY` placeholder and commented `CORS_ORIGINS` example.
  3. **nginx.conf.example** (new): Created example nginx reverse proxy config with all 4 service location blocks. Browser relay endpoint (`/browser-relay/`) is explicitly marked as public (no basic auth) — other services use `auth_basic`.
  4. **backup.sh**: Added `browser-relay` to DB_MAP (`browser-relay/browser-relay.db`) and default TARGETS array. Updated header comment.
  5. **upgrade.sh**: Added `browser-relay` to PORT_MAP (port 5013) and ALL_SERVICES array. Updated service ports documentation in header comment.
- Files changed: `deploy/docker-compose.monitoring.yml`, `deploy/secrets.env.example`, `deploy/nginx.conf.example` (new), `deploy/backup.sh`, `deploy/upgrade.sh`, `plan.md`
- **Learnings for future iterations:**
  - The `depends_on` with `condition: service_healthy` ensures browser-relay starts only after its upstream services (error-tracker, metrics-collector) pass their health checks — critical since browser-relay forwards to them immediately on startup
  - Browser-relay uses `ADMIN_API_KEY` (not `API_KEY`) — the compose environment mapping is `ADMIN_API_KEY=${BROWSER_RELAY_ADMIN_API_KEY}` rather than the `API_KEY=${SERVICE_API_KEY}` pattern used by the other three services
  - The nginx config is an example snippet (not a full nginx.conf) — users add these location blocks to their existing server block. Browser relay is the only endpoint without basic auth since browsers can't send basic auth headers

